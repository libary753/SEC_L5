import os
import json
from trie import Trie
from utils import load_janus, load_slideVQA_annotations, load_index
import torch
from tqdm import tqdm
from collections import Counter

# trie ë§Œë“¤ê¸°
def build_trie(vlm, tokenizer, vl_chat_processor):
    # Keywordë“¤ì„ encoding
    input_ids = []
    deck_ids = []
    for tag, deck_idx in tag_dict.items():
        input_id = vl_chat_processor.tokenizer.encode(tag)[1:]
        input_ids.append(input_id)
        deck_ids.append(deck_idx)
        
    trie = Trie(input_ids, deck_ids) # input_idsë¡œ trieë¥¼ ë§Œë“¤ê³ , leafì— deck_idsë¥¼ ì €ì¥í•¨
    return trie

def get_user_prompot(prompt):
    return {
        "role": "<|User|>",
        "content": f"{prompt}",
    }
    
def get_assistant_prompot(prompt):
    
    return {
        "role": "<|Assistant|>",
        "content": f"{prompt}",
    }

# Retrievalì„ ìœ„í•œ few shot samples
def get_few_shot_conversations():    
    conversation = []
    conversation.append(get_user_prompot("From now on, you are an assistant that helps with retrieval."))
    conversation.append(get_user_prompot("Query: How many online shopping orders are made per month in CY2013 in India?"))
    conversation.append(get_assistant_prompot("Tag: india, online shopping, CY2013"))
    conversation.append(get_user_prompot("Query: Which country is the researcher with the Twitter handle @doqtu84 giving a Group-buying Market Overview of?"))
    conversation.append(get_assistant_prompot("Tag: @doqtu84, Group-buying Market Overview"))
    conversation.append(get_user_prompot("Query: Which is most expensive among iOS, Android, and Windows?"))
    conversation.append(get_assistant_prompot("Tag: iOS, Android, Windows"))
    conversation.append(get_user_prompot("Query: What mobile game is shown in the presentation?"))
    conversation.append(get_assistant_prompot("Tag: mobile game"))
    conversation.append(get_user_prompot("Query: What is Fonny Schenck's phone number?"))
    conversation.append(get_assistant_prompot("Tag: Fonny Schenck"))
    conversation.append(get_user_prompot("Query: What is the structure into which glomeruler filtrate filters through the glomeruli part of?"))
    conversation.append(get_assistant_prompot("Tag: glomeruler, filtrate, filters"))
    return conversation

def generate_tags(
    query, 
    trie,
    vlm, 
    tokenizer, 
    vl_chat_processor,
    beam_size=16,
    max_tag_len=128,
    temperature=1,
    ):
    # 1. Conversation ìƒì„±
    # 1.1. Few-shot í”„ë¡¬í”„íŠ¸ ìƒì„±
    conversation = get_few_shot_conversations()
    # 1.2. query ì¶”ê°€
    conversation.append(get_user_prompot(f"Query: {query}"))
    # 1.3. Assistant prompt ì¶”ê°€
    conversation.append(get_assistant_prompot("Tag: "))

    # 2 Input ì „ì²˜ë¦¬
    # ì´ì œ ì´ë¯¸ì§€ê°€ ì—†ê¸° ë•Œë¬¸ì—, imagesì—ëŠ” ë¹ˆ listë¥¼ ë„˜ê²¨ì¤Œ
    prepare_inputs = vl_chat_processor(
        conversations=conversation, images=[], force_batchify=True
    ).to(vlm.device)
    inputs_embeds = vlm.prepare_inputs_embeds(**prepare_inputs)
    
    # 3. Inference ì¤‘ ê²°ê³¼ë¥¼ ë³´ê´€í•œ bufferë“¤
    # 3.1. í˜„ì¬ê¹Œì§€ generatedëœ í† í°ë“¤ ë³´ê´€
    generated_tokens = -torch.ones((beam_size, max_tag_len), dtype=torch.int).cuda()
    # 3.2. beam searchë¥¼ ìœ„í•œ log lickelihood ë²„í¼
    log_likelihoods = torch.zeros(beam_size, device=vlm.device)
    # 3.3. ëª¨ë“  beamì´ ë™ì‹œì— searchê°€ ëë‚˜ëŠ” ê²Œ ì•„ë‹˜ -> ì•„ì§ ìƒì„±ì´ ëë‚˜ì§€ ì•Šì€ beamì˜ ìˆ˜ ê´€ë¦¬
    beam_left = beam_size
    # 3.4. ìƒì„±ì´ ëë‚œ beamì˜ ê²°ê³¼ë¥¼ ë³´ê´€í•˜ê¸° ìœ„í•œ buffer
    generated_tags = []
    # 3.5. ëª¨ë“  beamì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ inputs embedsë¥¼ beam sizeë§Œí¼ ë³µì‚¬
    # TODO í•˜ì§€ë§Œ ì‹¤ì œë¡œëŠ” ì²˜ìŒì— ê°€ì¥ ì²« beamë§Œì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ì‚¬ì‹¤ ë¶ˆí•„ìš”í•œ ì—°ì‚°ëŸ‰ì„. ê°œì„  í•„ìš”
    inputs_embeds = inputs_embeds.repeat((beam_size, 1, 1))
    
    # 4. Trieë¥¼ ë”°ë¼ì„œ tag generation -> retrieval
    with torch.no_grad():
        for i in tqdm(range(max_tag_len)):
            # 4.1. Vlm inference
            # past_key_values: KV cachingì„ ìœ„í•´ í•„ìš”
            outputs = vlm.language_model.model(inputs_embeds=inputs_embeds, use_cache=True, past_key_values=outputs.past_key_values if i != 0 else None)
            
            # 4.2.log_prob ê³„ì‚°
            hidden_states = outputs.last_hidden_state            
            logits = vlm.language_model.lm_head(hidden_states[:, -1, :])
            probs = torch.softmax(logits / temperature, dim=-1)
            log_probs = torch.log(probs)
            
            # 4.3. log-likelihoods ê³„ì‚°
            log_probs = log_likelihoods.unsqueeze(-1) + log_probs            
            log_probs = log_probs.clamp_min(-1e10)
            if i == 0:
                log_probs[1:] = -torch.inf  # ê°€ì¥ ì²˜ìŒì—ëŠ” ê°€ì¥ ì²« sampleë§Œ ì‚¬ìš© -> ë‚˜ë¨¸ì§€ beam_size - 1ê°œë§Œí¼ì€ ë²„ë¦¬ëŠ”ê±°ë¼ì„œ ì²˜ìŒë¶€í„° ê³„ì‚° ì•ˆ í•´ë„ ë¨ -> ëŒ€ì‹  ì½”ë“œ ë³µì¡í•´ì ¸ì„œ ì¼ë‹¨ ë‘        
            log_probs = log_probs.flatten() # flatten
            
            # 4.4. Trieë¥¼ ê³ ë ¤í•˜ì—¬, trieì— ì—†ëŠ” next token ì„ íƒì§€ëŠ” log_probì„ -torch.infë¡œ ì„¤ì •í•´ì„œ, ë½‘íˆì§€ ì•Šê²Œ í•œë‹¤.
            # TODO í•˜ì§€ë§Œ, trieì˜ root ê·¼ì²˜ì—ì„œ beam_sizeë³´ë‹¤ ì‘ì€ ê°œìˆ˜ê°€ ì¡´ì¬í•˜ë©´, ë¬¸ì œê°€ ìƒê¹€. ì˜ˆì™¸ì²˜ë¦¬ë‚˜ ë‹¤ë¥¸ ë°©ë²• ì ìš© í•„ìš”
            dict_size = probs.shape[-1]
            trie_mask = torch.ones((beam_left, dict_size), dtype=torch.bool, device= vlm.device) # Trueë©´ trieì— ì—†ëŠ” ê²ƒ
            for j in range(beam_left):                
                next = trie.trie                  
                for k in range(i):
                    next = next.children[int(generated_tokens[j][k])]
                trie_mask[j][list(next.children.keys())] = False
            trie_mask = trie_mask.flatten()
            log_probs[trie_mask] = -torch.inf            
            
            # 4.5. top-k log-likelihoodsë¥¼ ê³ ë¥¸ë‹¤.
            log_probs_topk = log_probs.topk(beam_left)
            log_likelihoods = log_probs_topk.values
            indices = log_probs_topk.indices
            
            # 4.6. ëª‡ ë²ˆì§¸ beamì˜ ëª‡ ë²ˆì§¸ tokenì´ ë½‘í˜”ëŠ”ì§€ ê³„ì‚°
            beam_indices = indices // dict_size
            next_token = indices % dict_size
            
            # 4.7. generated_tokens ì—…ë°ì´íŠ¸
            # 4.7.1. ê¸°ì¡´ì˜ generated_tokensì˜ ìˆœì„œë„ ë³€ê²½í•´ì¤˜ì•¼í•¨
            generated_tokens = torch.stack([generated_tokens[beam_idx] for beam_idx in beam_indices])
            # 4.7.2. ê·¸ ë’¤ì— ì´ë²ˆì— ìƒì„±í•œ token ì¶”ê°€
            generated_tokens[:, i] = next_token.squeeze(dim=-1).to(torch.int32)
                        
            # 4.8. Trieì˜ ëì— ë„ì°©í•œ beamì€ ëë‚´ê³ , ë” ì´ìƒ searchë¥¼ í•˜ì§€ ì•Šê²Œ ë¹¼ì¤˜ì•¼í•¨
            _beam_left = beam_left
            next_beam_ids = []
            for beam_idx in range(_beam_left):
                beam_finished = False
                next = trie.trie
                for k in range(i+1):
                    next = next.children[int(generated_tokens[beam_idx][k])]
                    # í˜„ì¬ ì§„í–‰ëœ depthê¹Œì§€ ì™”ì„ ë•Œ, next childrenì´ ì—†ë‹¤ë©´, í•´ë‹¹ beamì€ Trieì˜ ëì— ë„ì°©í•œ ê²ƒì„.
                    if len(next.children) == 0:
                        beam_finished = True
                        break
                
                # í•´ë‹¹ beamì´ ëë‚¬ìœ¼ë©´, beam_leftë¥¼ 1 ê°ì†Œì‹œí‚¤ê³ , generated_tagsì— ê²°ê³¼ ì €ì¥
                if beam_finished:
                    beam_left -= 1
                    generated_tags.append(generated_tokens[beam_idx, :i+1])
                # í•´ë‹¹ beamì´ ëë‚˜ì§€ ì•Šì•˜ìœ¼ë©´, ë‹¤ìŒ stpe ì¤€ë¹„í•˜ê¸° ìœ„í•´ beam idxë¥¼ next_beam_idsì— ì €ì¥
                else:
                    next_beam_ids.append(beam_idx)
            
            # 4.9. ë‚¨ì€ beamì´ ìˆìœ¼ë©´, ë‹¤ìŒ Stepì„ ìœ„í•œ ì¤€ë¹”ë¥¼ í•´ì•¼í•¨           
            if beam_left > 0:                    
                # 4.9.1 next_token ì—…ë°ì´íŠ¸
                next_token = torch.stack([next_token[next_beam_id] for next_beam_id in next_beam_ids])
                # 4.9.2 log_likelihoods ì—…ë°ì´íŠ¸
                log_likelihoods = torch.stack([log_likelihoods[next_beam_id] for next_beam_id in next_beam_ids])
                # 4.9.3 generated tokensì—…ë°ì´íŠ¸
                generated_tokens = torch.stack([generated_tokens[next_beam_id] for next_beam_id in next_beam_ids])
                # 4.9.4 ì´ë²ˆì— ìƒì„±í•œ tokenì˜ ì„ë² ë”©ì„ ë‹¤ì‹œ ê³„ì‚°í•´ì„œ, inputs_embeds ì—…ë°ì´íŠ¸
                inputs_embeds = vlm.language_model.get_input_embeddings()(next_token).unsqueeze(dim=1)
                
                # KV cache ì—…ë°ì´íŠ¸
                for l, (key, value) in enumerate(zip(outputs.past_key_values.key_cache, outputs.past_key_values.value_cache)):
                    key_buffer = torch.zeros((beam_left, key.shape[1], key.shape[2], key.shape[3]), dtype=key.dtype, device=key.device)
                    value_buffer = torch.zeros((beam_left, value.shape[1], value.shape[2], value.shape[3]), dtype=value.dtype, device=value.device)                    
                    for j, next_beam_id in enumerate(next_beam_ids):
                        key_buffer[j] = key[beam_indices[next_beam_id]]        
                        value_buffer[j] = value[beam_indices[next_beam_id]]                    
                    outputs.past_key_values.key_cache[l] = key_buffer
                    outputs.past_key_values.value_cache[l] = value_buffer
            else:
                break
            
    return generated_tags

def retrieval(
    queries,
    trie,
    vlm, 
    tokenizer, 
    vl_chat_processor,
    ):
    top_1 = 0
    top_3 = 0
    top_10 = 0
    sampled = 0
    
    total = len(queries)
    
    # Queryë“¤ì— ëŒ€í•œ í‰ê°€
    for cnt, query in enumerate(queries):
        # 1. Queryë¡œë¶€í„° íƒœê·¸ ìƒì„±
        generated_tags = generate_tags(
            query["question"],
            trie,    
            vlm, 
            tokenizer, 
            vl_chat_processor,        
        )
        
        # 2. ì±„ì ì„ ìœ„í•´ ì´ë²ˆ deckì˜ deck idxë¥¼ ê°€ì ¸ì˜´
        deck_name = query['deck_name']
        deck_idx = deck_indices[deck_name]    
        
        # 3. ìƒì„±ëœ tagë“¤ì˜ trie leafì—ì„œ dataë¥¼ ê°€ì ¸ì˜´ -> tagì— í•´ë‹¹í•˜ëŠ” deck list
        # TODO: tag_dictì—ì„œ ë°”ë¡œ ë½‘ì•„ë‹¤ ì¨ë„ ë ê±°ê°™ì€ë°, ì™„ì „íˆ ë™ì¼í•œì§€ í™•ì¸ í•„ìš”
        tags = []
        for generated_ids in generated_tags:
            next = trie.trie
            for id in generated_ids:
                next = next.children[int(id)]    
            print#(next.data)
            tags += next.data
        
        # 4. ê°€ì¥ ë§ì´ ë‚˜ì˜¨ deckì´ ì •ë‹µ deckê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸ -> Top 1 recall
        count_dict = Counter(tags)  
        if deck_idx == max(count_dict, key=count_dict.get):
            top_1+=1
        top_3_preds = [deck for deck, _ in count_dict.most_common(3)]  # ğŸ”¹ ìƒìœ„ 3ê°œ ì˜ˆì¸¡
        print(top_3_preds)
        if deck_idx in top_3_preds:
            top_3 += 1
        top_10_preds = [deck for deck, _ in count_dict.most_common(10)]  # ğŸ”¹ ìƒìœ„ 3ê°œ ì˜ˆì¸¡
        print(top_10_preds)
        if deck_idx in top_10_preds:
            top_10 += 1
        # 6. ì •ë‹µ deckì´ candidatesì— ìˆëŠ”ì§€ í™•ì¸ -> In candidates
        if deck_idx in count_dict:
            sampled+=1
        
        print(f"Idx: {cnt+1} Deck idx: {deck_idx} total: {total}")
        print(f"Top 1: {top_1/(cnt+1)*100:0.2f}")
        print(f"Top 3: {top_3/(cnt+1)*100:0.2f}")
        print(f"Top 10: {top_10/(cnt+1)*100:0.2f}")
        print(f"In candidates: {sampled/(cnt+1)*100:0.2f}")
        print(f"-------------------------------------")

    # ê²°ê³¼ í‘œì‹œ
    print(f"Total: {total}")
    print(f"Top 1: {top_1/total*100:0.2f}")
    print(f"Top 3: {top_3/total*100:0.2f}")
    print(f"Top 10: {top_10/total*100:0.2f}")
    print(f"In candidates: {sampled/total*100:0.2f}")

if __name__ == "__main__":
    # 1. ì¸ë±ì‹±í•´ë‘” ë°ì´í„°ì…‹ ë¡œë“œ
    tag_dict, deck_dict, decks, deck_indices = load_index("output/indexing")
    
    # 2. Trie ìƒì„±
    # 2.1 Janus ë¡œë“œ
    vlm, tokenizer, vl_chat_processor = load_janus()
    trie = build_trie(vlm, tokenizer, vl_chat_processor)
    
    # 3. Annotation ë°ì´í„°ì…‹ ë¡œë“œ
    queries = load_slideVQA_annotations(decks, "SlideVQA", "test")
    
    # 4. queryë“¤ì— ëŒ€í•´ tag ìƒì„±í•˜ê³ , tagë“¤ì„ í¬í•¨í•˜ëŠ” deckë“¤ì„ ì´ìš©í•˜ì—¬ ì„±ëŠ¥ ì§‘ê³„
    retrieval(queries, trie, vlm, tokenizer, vl_chat_processor)